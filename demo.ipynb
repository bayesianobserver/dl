{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from utils import set_seed\n",
    "import sys\n",
    "set_seed(3407)\n",
    "import pylab as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 12:31:47.681099: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-30 12:31:47.843995: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-04-30 12:31:47.844015: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-04-30 12:31:48.447840: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-30 12:31:48.447906: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-30 12:31:48.447912: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class TabularRegressionData(Dataset):\n",
    "    \"\"\" \n",
    "    Simple tabular data with 5 numerical features and 1 numerical response. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, dimensions = 5):\n",
    "        assert split in {'train', 'test'}\n",
    "        self.split = split\n",
    "        self.dimensions = dimensions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 10000 # ...\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.randn(5)\n",
    "        y = -3.15 + x[0] + (2 * x[2] * x[3]) - 3 * (x[4]**3)\n",
    "        x, y\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVLoader(Dataset):\n",
    "    def __init__(self, file_path, feature_cols, target_col, dimensions = 5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_path (str): Path to the CSV file.\n",
    "            feature_cols (list of str): List of column names to be used as features.\n",
    "            target_col (str): Column name of the target variable.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(file_path)\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_col = target_col\n",
    "        self.dimensions = dimensions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        features = torch.tensor(self.data_frame.loc[idx, self.feature_cols].values.astype('float32'))\n",
    "        target = torch.tensor(self.data_frame.loc[idx, self.target_col], dtype = torch.float32)\n",
    "        return features, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0971,  0.5328, -0.3311, -0.4458,  0.6679]) tensor(-2.6515)\n",
      "tensor([ 0.7001, -0.8284,  2.1086, -1.0221, -1.3357]) tensor(0.3886)\n"
     ]
    }
   ],
   "source": [
    "# print an example instance of the dataset\n",
    "train_dataset = TabularRegressionData('train')\n",
    "# train_dataset = CSVLoader(\n",
    "#         file_path='/home/suhas/research/dl/training_data.txt',  # Specify your CSV file path here\n",
    "#         feature_cols=['x0','x1','x2','x3','x4'],  # Specify the feature column names\n",
    "#         target_col='y'  # Specify the target column name\n",
    "#     )\n",
    "\n",
    "test_dataset = TabularRegressionData('test')\n",
    "x, y = train_dataset[0]\n",
    "print(x, y)\n",
    "x1, y1 = test_dataset[0]\n",
    "print(x1, y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(train_dataset), sys.getsizeof(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32, torch.float32, torch.float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.dtype, x.dtype, y1.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a GPT instance\n",
    "from model_ff import ffmodel\n",
    "model_config = ffmodel.get_default_config()\n",
    "model_config.input_size = 5\n",
    "model_config.use_dropout = True\n",
    "model_config.p_drop = 0.1\n",
    "model = ffmodel(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cpu\n"
     ]
    }
   ],
   "source": [
    "# create a Trainer object\n",
    "from trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 3e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 20000\n",
    "train_config.num_workers = 0\n",
    "train_config.batch_size = 100\n",
    "train_config.eval_iters = 100\n",
    "train_config.resume = False\n",
    "train_config.checkpoint_path = '/home/suhas/research/dl/checkpoints/'\n",
    "trainer = Trainer(train_config, model, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_end_callback(trainer):\n",
    "#     if trainer.iter_num % 100 == 0:\n",
    "#         print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "        \n",
    "# #trainer.set_callback('on_batch_end', batch_end_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_num 100  train_loss: 164.03941345214844 , test_loss:  154.05628967285156 , last batch loss: 381.0360107421875\n",
      "iter_num 200  train_loss: 140.9652862548828 , test_loss:  150.99436950683594 , last batch loss: 69.68637084960938\n",
      "iter_num 300  train_loss: 142.23208618164062 , test_loss:  136.8008270263672 , last batch loss: 68.88346862792969\n",
      "iter_num 400  train_loss: 132.6568603515625 , test_loss:  126.99662017822266 , last batch loss: 73.5093994140625\n",
      "iter_num 500  train_loss: 122.76705932617188 , test_loss:  144.32217407226562 , last batch loss: 364.3342590332031\n",
      "Directory created at /home/suhas/research/dl/checkpoints/\n",
      "Checkpoint saved at iter 500\n",
      "iter_num 600  train_loss: 128.3727264404297 , test_loss:  123.9771499633789 , last batch loss: 77.46800231933594\n",
      "iter_num 700  train_loss: 110.67684173583984 , test_loss:  112.02320098876953 , last batch loss: 121.87883758544922\n",
      "iter_num 800  train_loss: 90.00758361816406 , test_loss:  98.77197265625 , last batch loss: 53.19053649902344\n",
      "iter_num 900  train_loss: 97.85456085205078 , test_loss:  97.06780242919922 , last batch loss: 199.69679260253906\n",
      "iter_num 1000  train_loss: 90.12635803222656 , test_loss:  90.8958969116211 , last batch loss: 82.7743911743164\n",
      "Directory already exists at /home/suhas/research/dl/checkpoints/\n",
      "Checkpoint saved at iter 1000\n"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = pd.DataFrame(trainer.train_losses)\n",
    "dftrain.columns = ['batches','loss']\n",
    "\n",
    "dftest = pd.DataFrame(trainer.test_losses)\n",
    "dftest.columns = ['batches','loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(dftrain.batches), list(dftrain.loss))\n",
    "plt.plot(list(dftest.batches), list(dftest.loss))\n",
    "plt.legend(['train loss','test_loss'])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0971,  0.5328, -0.3311, -0.4458,  0.6679])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0971,  0.5328, -0.3311, -0.4458,  0.6679])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0971,  0.5328, -0.3311, -0.4458,  0.6679])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0971,  0.5328, -0.3311, -0.4458,  0.6679])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3041621148586273"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
