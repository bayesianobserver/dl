{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from utils import set_seed\n",
    "import sys\n",
    "set_seed(3407)\n",
    "import pylab as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 12:31:47.681099: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-30 12:31:47.843995: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-04-30 12:31:47.844015: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-04-30 12:31:48.447840: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-30 12:31:48.447906: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-30 12:31:48.447912: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class TabularRegressionData(Dataset):\n",
    "    \"\"\" \n",
    "    Simple tabular data with 5 numerical features and 1 numerical response. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, dimensions = 5):\n",
    "        assert split in {'train', 'test'}\n",
    "        self.split = split\n",
    "        self.dimensions = dimensions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 10000 # ...\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.randn(5)\n",
    "        y = -3.15 + x[0] + (2 * x[2] * x[3]) - 3 * (x[4]**3)\n",
    "        x, y\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVLoader(Dataset):\n",
    "    def __init__(self, file_path, feature_cols, target_col, dimensions = 5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_path (str): Path to the CSV file.\n",
    "            feature_cols (list of str): List of column names to be used as features.\n",
    "            target_col (str): Column name of the target variable.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(file_path)\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_col = target_col\n",
    "        self.dimensions = dimensions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        features = torch.tensor(self.data_frame.loc[idx, self.feature_cols].values.astype('float32'))\n",
    "        target = torch.tensor(self.data_frame.loc[idx, self.target_col], dtype = torch.float32)\n",
    "        return features, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0971,  0.5328, -0.3311, -0.4458,  0.6679]) tensor(-2.6515)\n",
      "tensor([ 0.7001, -0.8284,  2.1086, -1.0221, -1.3357]) tensor(0.3886)\n"
     ]
    }
   ],
   "source": [
    "# print an example instance of the dataset\n",
    "train_dataset = TabularRegressionData('train')\n",
    "# train_dataset = CSVLoader(\n",
    "#         file_path='/home/suhas/research/dl/training_data.txt',  # Specify your CSV file path here\n",
    "#         feature_cols=['x0','x1','x2','x3','x4'],  # Specify the feature column names\n",
    "#         target_col='y'  # Specify the target column name\n",
    "#     )\n",
    "\n",
    "test_dataset = TabularRegressionData('test')\n",
    "x, y = train_dataset[0]\n",
    "print(x, y)\n",
    "x1, y1 = test_dataset[0]\n",
    "print(x1, y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(train_dataset), sys.getsizeof(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32, torch.float32, torch.float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.dtype, x.dtype, y1.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a GPT instance\n",
    "from model_ff import ffmodel\n",
    "model_config = ffmodel.get_default_config()\n",
    "model_config.input_size = 5\n",
    "model_config.use_dropout = True\n",
    "model_config.p_drop = 0.1\n",
    "model = ffmodel(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cpu\n"
     ]
    }
   ],
   "source": [
    "# create a Trainer object\n",
    "from trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 3e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 20000\n",
    "train_config.num_workers = 0\n",
    "train_config.batch_size = 100\n",
    "train_config.eval_iters = 100\n",
    "train_config.resume = False\n",
    "train_config.checkpoint_path = '/home/suhas/research/dl/checkpoints/'\n",
    "trainer = Trainer(train_config, model, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_end_callback(trainer):\n",
    "#     if trainer.iter_num % 100 == 0:\n",
    "#         print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "        \n",
    "# #trainer.set_callback('on_batch_end', batch_end_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_num 100  train_loss: 164.03941345214844 , test_loss:  154.05628967285156 , last batch loss: 381.0360107421875\n",
      "iter_num 200  train_loss: 140.9652862548828 , test_loss:  150.99436950683594 , last batch loss: 69.68637084960938\n",
      "iter_num 300  train_loss: 142.23208618164062 , test_loss:  136.8008270263672 , last batch loss: 68.88346862792969\n",
      "iter_num 400  train_loss: 132.6568603515625 , test_loss:  126.99662017822266 , last batch loss: 73.5093994140625\n",
      "iter_num 500  train_loss: 122.76705932617188 , test_loss:  144.32217407226562 , last batch loss: 364.3342590332031\n",
      "Directory created at /home/suhas/research/dl/checkpoints/\n",
      "Checkpoint saved at iter 500\n",
      "iter_num 600  train_loss: 128.3727264404297 , test_loss:  123.9771499633789 , last batch loss: 77.46800231933594\n",
      "iter_num 700  train_loss: 110.67684173583984 , test_loss:  112.02320098876953 , last batch loss: 121.87883758544922\n",
      "iter_num 800  train_loss: 90.00758361816406 , test_loss:  98.77197265625 , last batch loss: 53.19053649902344\n",
      "iter_num 900  train_loss: 97.85456085205078 , test_loss:  97.06780242919922 , last batch loss: 199.69679260253906\n",
      "iter_num 1000  train_loss: 90.12635803222656 , test_loss:  90.8958969116211 , last batch loss: 82.7743911743164\n",
      "Directory already exists at /home/suhas/research/dl/checkpoints/\n",
      "Checkpoint saved at iter 1000\n",
      "iter_num 1100  train_loss: 84.13307189941406 , test_loss:  82.79309844970703 , last batch loss: 138.64234924316406\n",
      "iter_num 1200  train_loss: 77.85566711425781 , test_loss:  75.67707824707031 , last batch loss: 57.22773361206055\n",
      "iter_num 1300  train_loss: 64.30669403076172 , test_loss:  72.19857025146484 , last batch loss: 32.51708221435547\n",
      "iter_num 1400  train_loss: 70.5920181274414 , test_loss:  66.98188781738281 , last batch loss: 36.6775016784668\n",
      "iter_num 1500  train_loss: 77.92679595947266 , test_loss:  65.01179504394531 , last batch loss: 223.07958984375\n",
      "Directory already exists at /home/suhas/research/dl/checkpoints/\n",
      "Checkpoint saved at iter 1500\n",
      "iter_num 1600  train_loss: 64.89811706542969 , test_loss:  64.51141357421875 , last batch loss: 282.7393798828125\n",
      "iter_num 1700  train_loss: 61.196441650390625 , test_loss:  68.18061065673828 , last batch loss: 37.90419387817383\n",
      "iter_num 1800  train_loss: 60.31651306152344 , test_loss:  68.90125274658203 , last batch loss: 248.6813507080078\n",
      "iter_num 1900  train_loss: 57.42509078979492 , test_loss:  61.61622619628906 , last batch loss: 77.29003143310547\n",
      "iter_num 2000  train_loss: 56.35138702392578 , test_loss:  64.89403533935547 , last batch loss: 15.77099895477295\n",
      "Directory already exists at /home/suhas/research/dl/checkpoints/\n",
      "Checkpoint saved at iter 2000\n",
      "iter_num 2100  train_loss: 53.72772216796875 , test_loss:  59.92634582519531 , last batch loss: 81.1119384765625\n",
      "iter_num 2200  train_loss: 54.676063537597656 , test_loss:  46.279327392578125 , last batch loss: 57.394039154052734\n",
      "iter_num 2300  train_loss: 50.63962936401367 , test_loss:  47.68612289428711 , last batch loss: 43.76546859741211\n",
      "iter_num 2400  train_loss: 47.776615142822266 , test_loss:  49.77848815917969 , last batch loss: 71.08338928222656\n",
      "iter_num 2500  train_loss: 59.0698127746582 , test_loss:  47.60698318481445 , last batch loss: 99.3015365600586\n",
      "Directory already exists at /home/suhas/research/dl/checkpoints/\n",
      "Checkpoint saved at iter 2500\n",
      "iter_num 2600  train_loss: 49.28951644897461 , test_loss:  48.408546447753906 , last batch loss: 25.620468139648438\n",
      "iter_num 2700  train_loss: 44.56494140625 , test_loss:  47.067626953125 , last batch loss: 25.399229049682617\n",
      "iter_num 2800  train_loss: 42.80256271362305 , test_loss:  46.60332107543945 , last batch loss: 37.33736038208008\n",
      "iter_num 2900  train_loss: 34.210453033447266 , test_loss:  40.57084655761719 , last batch loss: 36.652164459228516\n",
      "iter_num 3000  train_loss: 35.647911071777344 , test_loss:  34.498680114746094 , last batch loss: 14.112548828125\n",
      "Directory already exists at /home/suhas/research/dl/checkpoints/\n",
      "Checkpoint saved at iter 3000\n",
      "iter_num 3100  train_loss: 35.85896301269531 , test_loss:  45.307926177978516 , last batch loss: 10.940577507019043\n",
      "iter_num 3200  train_loss: 39.3808479309082 , test_loss:  35.964481353759766 , last batch loss: 40.019432067871094\n",
      "iter_num 3300  train_loss: 31.93172264099121 , test_loss:  40.03535461425781 , last batch loss: 56.5374870300293\n",
      "iter_num 3400  train_loss: 30.787166595458984 , test_loss:  36.37804412841797 , last batch loss: 15.48463249206543\n",
      "iter_num 3500  train_loss: 39.05803680419922 , test_loss:  34.140838623046875 , last batch loss: 26.99337387084961\n",
      "Directory already exists at /home/suhas/research/dl/checkpoints/\n",
      "Checkpoint saved at iter 3500\n",
      "iter_num 3600  train_loss: 25.96181869506836 , test_loss:  30.930906295776367 , last batch loss: 89.6061782836914\n",
      "iter_num 3700  train_loss: 30.63443946838379 , test_loss:  31.105426788330078 , last batch loss: 29.376256942749023\n",
      "iter_num 3800  train_loss: 29.600370407104492 , test_loss:  30.748640060424805 , last batch loss: 13.915467262268066\n",
      "iter_num 3900  train_loss: 22.510326385498047 , test_loss:  38.46775817871094 , last batch loss: 177.65487670898438\n",
      "iter_num 4000  train_loss: 24.269622802734375 , test_loss:  24.59708023071289 , last batch loss: 222.9439239501953\n",
      "Directory already exists at /home/suhas/research/dl/checkpoints/\n",
      "Checkpoint saved at iter 4000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/research/dl/trainer.py:143\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# fetch the next batch (x, y) and re-init iterator if needed\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     data_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_loader)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m, in \u001b[0;36mTabularRegressionData.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3.15\u001b[39m \u001b[38;5;241m+\u001b[39m x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m x[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m x[\u001b[38;5;241m3\u001b[39m]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m (x[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     19\u001b[0m     x, y\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = pd.DataFrame(trainer.train_losses)\n",
    "dftrain.columns = ['batches','loss']\n",
    "\n",
    "dftest = pd.DataFrame(trainer.test_losses)\n",
    "dftest.columns = ['batches','loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(dftrain.batches), list(dftrain.loss))\n",
    "plt.plot(list(dftest.batches), list(dftest.loss))\n",
    "plt.legend(['train loss','test_loss'])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0971,  0.5328, -0.3311, -0.4458,  0.6679])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0971,  0.5328, -0.3311, -0.4458,  0.6679])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0971,  0.5328, -0.3311, -0.4458,  0.6679])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0971,  0.5328, -0.3311, -0.4458,  0.6679])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3041621148586273"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
